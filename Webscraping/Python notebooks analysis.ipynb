{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54dfc5b",
   "metadata": {},
   "source": [
    "# Python notebooks analysis with Azure Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96b1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import openai\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "from langchain.vectorstores import AzureSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efcce6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9ab594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 23-Oct-2023 09:37:12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Today is {datetime.datetime.today().strftime('%d-%b-%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d39276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open AI version: 0.28.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Open AI version:\", openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e03c7",
   "metadata": {},
   "source": [
    "## Web page to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab90ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/Azure/azure-openai-samples/blob/main/use_cases/call_center/notebooks/call_center.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7e442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"notebooks\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3524edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = text.replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\")\n",
    "    # Saving the extracted text as a text file\n",
    "    text_file = os.path.join(DATA_DIR, url.split(\"/\")[-1] + \".txt\")\n",
    "    with open(text_file, \"w\") as file:\n",
    "        file.write(text)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d21a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\r\n",
      "-rwxrwxrwx 1 root root 16K Oct 23 09:37 call_center.ipynb.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA_DIR -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cea8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file {file}\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for file in [f for f in os.listdir(DATA_DIR) if f.endswith(\".txt\")]:\n",
    "    print(\"Processing file {file}\")\n",
    "    try:\n",
    "        loader = TextLoader(os.path.join(DATA_DIR, file), encoding=\"utf-8\")\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8dccea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbc735",
   "metadata": {},
   "source": [
    "## Azure Cognitive Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bfbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"webcrawler-url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ebca75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"azure.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5780c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our embedding model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    deployment=os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    model=os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\"),\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_type=\"azure\",\n",
    "    chunk_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1b2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our Azure Search\n",
    "acs = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_COGNITIVE_SEARCH_API_KEY\"),\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab37b222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YzA5OTA1MDMtYTE2MS00ZjdhLWE1MDEtNzNiOGQ5ZjVjZmU1',\n",
       " 'MDFmZDU0MjgtMjdiMS00MmY5LWEzYTQtYThlNTUyMWVkYWI0',\n",
       " 'NDhlNTFlZTQtMGY1Zi00MGY0LTkzODEtOGY0ZmJhZjFhMmNm',\n",
       " 'MDYzOWNjMjItNGY1Zi00YjRhLWJiNDQtZDMwM2EzYzQ2YzZj',\n",
       " 'MGQwYmRmZDgtNTkxMy00NjJmLWJjOTMtYjIxNWZiOGY2ZTI2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to Azure Search\n",
    "acs.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28afc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Azure Cognitive Search as our retriever\n",
    "retriever = AzureCognitiveSearchRetriever(\n",
    "    content_key=\"content\", top_k=5, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950629e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chatGPT 3.5 as our LLM\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo-16k\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fd5b7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2848f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a template message\n",
    "template = \"\"\"\n",
    "You are an AI Python expert powered by Azure Open AI.\n",
    "You are going to analyse some python code.\n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Set the Retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dd20ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa():\n",
    "    \"\"\"\n",
    "    Get answer\n",
    "    \"\"\"\n",
    "    result = qa_chain({\"query\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result))\n",
    "    source = result[\"source_documents\"][0].metadata[\"metadata\"]\n",
    "\n",
    "    print(f\"Question: {question} \\n\")\n",
    "    print(f\"Answer: {result['result']} \\n\")\n",
    "    print(f\"{source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe78f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2343bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who are you? \n",
      "\n",
      "Answer: I am an AI Python expert powered by Azure Open AI. I am here to help you with any Python code analysis or questions you may have. Thanks for asking! \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Who are you?\"\n",
    "\n",
    "res = qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1352e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you summary this code? \n",
      "\n",
      "Answer: This Python code appears to be a Jupyter notebook that focuses on post-call transcription and analysis in a call center scenario using Azure OpenAI services. The code performs sentiment analysis and summarization on call center transcriptions.\n",
      "\n",
      "The notebook is divided into different sections, each addressing a specific task. Here is a summary of each section:\n",
      "\n",
      "1. Introduction: Provides an overview of the call center case and the tasks to be performed.\n",
      "\n",
      "2. Environment Setup: Loads the necessary libraries and sets up the environment by loading the .env file.\n",
      "\n",
      "3. Speech Recognition: Defines a function `recognize_speech_from_file` that uses Azure Cognitive Services Speech SDK to transcribe customer call recordings into text.\n",
      "\n",
      "4. Sentiment Analysis: Transcribes a customer call from a WAV file (`good_review.wav`) and prints the transcribed text. Then, it creates a prompt for sentiment analysis using OpenAI's GPT-3 model and sends the prompt to OpenAI's API to detect the customer's sentiment.\n",
      "\n",
      "5. Negative Example: Similar to the previous section, but uses a different WAV file (`bad_review.wav`) to transcribe a negative customer call and detect sentiment.\n",
      "\n",
      "6. Summarization: Transcribes a customer call from a WAV file (`good_review.wav`) and prints the transcribed text. Then, it creates a prompt for summarization using OpenAI's GPT-3 model and sends the prompt to OpenAI's API to generate a summary of the customer message.\n",
      "\n",
      "Overall, the code demonstrates how to use Azure Cognitive Services Speech SDK for speech recognition and OpenAI's GPT-3 model for sentiment analysis and summarization in a call center scenario.\n",
      "\n",
      "Thanks for asking! \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you summary this code?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab78382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many steps in this code? \n",
      "\n",
      "Answer: There are 14 steps in this code. \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"How many steps in this code?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f6d5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you describe these steps? \n",
      "\n",
      "Answer: Based on the provided information, it seems that the code is part of a Jupyter Notebook file (call_center.ipynb) hosted on GitHub. The code snippet provided is not visible, so it is not possible to describe the specific steps or analyze the Python code.\n",
      "\n",
      "If you have the actual code snippet or any specific questions about Python code analysis, please provide it, and I'll be happy to help you. Thanks for asking! \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you describe these steps?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0c8f13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you optimize the step 1? \n",
      "\n",
      "Answer: In step 1, the code is loading the environment variables from a .env file using the `dotenv` library. It then sets the OpenAI API key and base URL using the loaded environment variables.\n",
      "\n",
      "The code can be optimized by removing the unnecessary imports and reorganizing the code for better readability. Here's an optimized version of step 1:\n",
      "\n",
      "```python\n",
      "from dotenv import load_dotenv\n",
      "from pathlib import Path\n",
      "\n",
      "env_path = Path('../../../.env')  # Change with your .env file\n",
      "load_dotenv(dotenv_path=env_path, override=True)\n",
      "\n",
      "import os\n",
      "import openai\n",
      "\n",
      "openai.api_type = \"azure\"\n",
      "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
      "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
      "openai.api_version = \"2022-06-01-preview\"\n",
      "\n",
      "SPEECH_KEY = os.environ[\"SPEECH_API_KEY\"]\n",
      "COMPLETIONS_MODEL = os.environ[\"COMPLETIONS_MODEL\"]\n",
      "```\n",
      "\n",
      "This optimized code removes the unnecessary imports and organizes the code in a more readable way. It also removes the duplicate import of the `os` module.\n",
      "\n",
      "Thanks for asking! \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you optimize the step 1?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5c31d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you optimize the extract metadata step? \n",
      "\n",
      "Answer: To optimize the extract metadata step, you can consider the following:\n",
      "\n",
      "1. Use a more efficient method to extract metadata from the code. Instead of parsing the code manually, you can use existing Python libraries such as `ast` or `inspect` to extract metadata like function names, variable names, and imports.\n",
      "\n",
      "2. Use caching or memoization techniques to store the extracted metadata. If the code analysis is performed frequently on the same code, you can cache the metadata to avoid redundant analysis and improve performance.\n",
      "\n",
      "3. Parallelize the metadata extraction process. If you have a large codebase or multiple code files to analyze, you can parallelize the extraction process by using multiprocessing or threading techniques. This can help utilize multiple CPU cores and speed up the analysis.\n",
      "\n",
      "4. Optimize the data structures used to store the metadata. Depending on the specific requirements of your analysis, you can choose appropriate data structures like dictionaries, sets, or lists to efficiently store and access the extracted metadata.\n",
      "\n",
      "By implementing these optimizations, you can improve the efficiency and performance of the extract metadata step in your code analysis process.\n",
      "\n",
      "Thanks for asking! \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you optimize the extract metadata step?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25ca5565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you explain the speech to text step? \n",
      "\n",
      "Answer: The speech to text step in the provided Python code is performed using the Azure Cognitive Services Speech SDK. Here's a breakdown of the code:\n",
      "\n",
      "1. The function `recognize_speech_from_file(filename)` is defined to transcribe the speech from an audio file to text.\n",
      "2. The function takes the filename of the audio file as input.\n",
      "3. It sets up the subscription info for the Speech Service, including the speech key and service region.\n",
      "4. It creates a `SpeechConfig` object and an `AudioConfig` object using the provided filename.\n",
      "5. It creates a `SpeechRecognizer` object using the speech config and audio config.\n",
      "6. It defines two callback functions: `stop_cb` and `recognize_cb`.\n",
      "   - `stop_cb` is called when the speech recognition session is stopped and sets the `done` flag to True.\n",
      "   - `recognize_cb` is called when speech is recognized and appends the recognized text to the `recognized_text_list`.\n",
      "7. It connects the callback functions to the corresponding events of the speech recognizer.\n",
      "8. It starts continuous speech recognition using `speech_recognizer.start_continuous_recognition()`.\n",
      "9. It enters a loop and waits until the `done` flag is set to True.\n",
      "10. It stops continuous speech recognition using `speech_recognizer.stop_continuous_recognition()`.\n",
      "11. It returns the `recognized_text_list`, which contains the transcribed text.\n",
      "\n",
      "This step allows you to convert speech from an audio file into text, which can then be used for further analysis or processing. \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you explain the speech to text step?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "380ac033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is it possible to use another Azure technology for this step? \n",
      "\n",
      "Answer: Yes, it is possible to use another Azure technology for this step. Azure provides a wide range of services and technologies that can be integrated into your Python code. Depending on your specific requirements, you can explore options such as Azure Cognitive Services for speech recognition and sentiment analysis, Azure Text Analytics for text analysis, or Azure Machine Learning for building and deploying machine learning models. These services can be easily accessed and integrated into your Python code using the appropriate SDKs and APIs provided by Azure. \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Is it possible to use another Azure technology for this step?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5fa09e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are you main recommandation to improve all this code? \n",
      "\n",
      "Answer: Based on the provided code, here are some recommendations to improve it:\n",
      "\n",
      "1. Use meaningful variable and function names: It is important to use descriptive names for variables and functions to make the code more readable and understandable. For example, instead of using generic names like `text`, `prompt`, or `recognize_speech_from_file`, consider using more specific names that convey their purpose.\n",
      "\n",
      "2. Organize code into functions or classes: Breaking down the code into smaller functions or classes can improve modularity and reusability. It also makes the code easier to understand and maintain. Consider grouping related code together and encapsulating functionality into separate functions or classes.\n",
      "\n",
      "3. Add comments and docstrings: Comments and docstrings help explain the purpose and functionality of the code. They make it easier for other developers (including yourself) to understand the code and its intended usage. Consider adding comments to explain complex logic or provide context for certain code sections.\n",
      "\n",
      "4. Handle exceptions and errors: It is important to handle exceptions and errors gracefully to prevent the code from crashing or producing unexpected results. Use try-except blocks to catch and handle exceptions, and provide appropriate error messages or fallback behavior.\n",
      "\n",
      "5. Optimize code performance: Analyze the code for any potential performance bottlenecks and optimize them if necessary. This could involve using more efficient algorithms, reducing unnecessary computations, or optimizing data structures.\n",
      "\n",
      "6. Use version control: Consider using version control (such as Git) to track changes to the code and collaborate with other developers. This allows you to easily revert changes, track the history of the code, and work on different features or bug fixes simultaneously.\n",
      "\n",
      "7. Follow coding conventions and best practices: Adhere to coding conventions and best practices for Python, such as using consistent indentation, following naming conventions (e.g., using lowercase with underscores for variable names), and organizing imports properly.\n",
      "\n",
      "These recommendations should help improve the code's readability, maintainability, and performance. Remember to thoroughly test the code after making any changes to ensure its correctness. Thanks for asking! \n",
      "\n",
      "{\"source\": \"notebooks/call_center.ipynb.txt\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"What are you main recommandation to improve all this code?\"\n",
    "\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f119e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f558f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
